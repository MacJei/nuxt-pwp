{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_url = \"https://nagornyy.me/datasets/wine_reviews.csv.zip\"\n",
    "df_wine = pd.read_csv(wine_url)\n",
    "# удаляем вина без рейтинга или без цены\n",
    "df_wine.dropna(subset=[\"points\", \"price\"], inplace=True)\n",
    "y_bin = (df_wine.points > df_wine.points.median()).values.reshape(-1, 1)\n",
    "X = np.log(df_wine.price.values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7203256049503224"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_sklearn = LogisticRegression()\n",
    "log_reg_sklearn.fit(X, y_bin);\n",
    "predicted = log_reg_sklearn.predict(X)\n",
    "roc_auc_score(y_bin, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_model(X, w, b):\n",
    "    return sigmoid(X @ w.t() + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$binary \\ cross \\ entropy= \\frac{-1}{n} \\sum_{n}y\\cdot ln(\\hat{y})+(1-y)\\cdot ln(1-\\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def binary_cross_entropy(true, predicted):\n",
    "    return -1 / len(true) * (true * predicted.log() + (1 - true) * (1 - predicted).log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = torch.from_numpy(y_bin).float()\n",
    "X_tensor = torch.from_numpy(X).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8127689987977705"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# по мотивами Kaiming initialization\n",
    "w = torch.randn(y_bin_tensor.shape[1], X_tensor.shape[1]) / math.sqrt(2 / y_bin_tensor.shape[1])\n",
    "b = torch.zeros(y_bin_tensor.shape[1])\n",
    "\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True);\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    predictions = log_reg_model(X_tensor, w, b)\n",
    "    loss = binary_cross_entropy(predictions, sigmoid(y_bin_tensor))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "predictions = log_reg_model(X_tensor, w, b)\n",
    "roc_auc_score(y_bin, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8127689987977705"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8127689987977705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия с категориальными переменными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18198, 8440)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    \"country\",\n",
    "    \"region_1\",\n",
    "    \"region_2\",\n",
    "    \"variety\",\n",
    "    \"winery\",\n",
    "    \"province\"\n",
    "]\n",
    "X_sparse = pd.get_dummies(\n",
    "    df_wine[[\"price\", \"points\"] + categorical_features],\n",
    "    columns=categorical_features,\n",
    "    drop_first=True,\n",
    "    dummy_na=True\n",
    ")\n",
    "X_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse_tensor = torch.from_numpy(X_sparse.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# по мотивами Kaiming initialization\n",
    "w = torch.randn(y_tensor.shape[1], X_sparse_tensor.shape[1]) / math.sqrt(2 / y_tensor.shape[1])\n",
    "b = torch.zeros(y_tensor.shape[1])\n",
    "\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True);\n",
    "\n",
    "for i in range(100):\n",
    "    predictions = log_reg_model(X_sparse_tensor, w, b)\n",
    "    loss = binary_cross_entropy(predictions, sigmoid(y_bin_train_tensor))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "predictions = log_reg_model(X_sparse_tensor, w, b)\n",
    "roc_auc_score(y_bin_train, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Взаимодействие признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2 = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (18198, 35604141) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-578f8f2cd2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minteractions2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"country_Australia\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1587\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m                 XP = np.empty((n_samples, self.n_output_features_),\n\u001b[0;32m-> 1589\u001b[0;31m                               dtype=X.dtype, order=self.order)\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m                 \u001b[0;31m# What follows is a faster implementation of:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (18198, 35604141) and data type float64"
     ]
    }
   ],
   "source": [
    "interactions2 = poly2.fit_transform(X_sparse.loc[:, \"country_Australia\":].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM\n",
    "\n",
    "$$\\begin{split}\\begin{aligned}\n",
    "&\\sum_{i=1}^d \\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j \\\\\n",
    " &= \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j - \\frac{1}{2}\\sum_{i=1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_i\\rangle x_i x_i \\\\\n",
    " &= \\frac{1}{2} \\big (\\sum_{i=1}^d \\sum_{j=1}^d \\sum_{l=1}^k\\mathbf{v}_{i, l} \\mathbf{v}_{j, l} x_i x_j - \\sum_{i=1}^d \\sum_{l=1}^k \\mathbf{v}_{i, l} \\mathbf{v}_{j, l} x_i x_i \\big)\\\\\n",
    " &=  \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i) (\\sum_{j=1}^d \\mathbf{v}_{j, l}x_j) - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2 \\big ) \\\\\n",
    " &= `\\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)\n",
    " \\end{aligned}\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\textbf v_i, \\textbf v_j \\rangle x_i x_j = \n",
    "\\frac{1}{2} \\sum_{f=1}^{k} \\Big( \\big(\\sum_{i=1}^{n} v_f^{(i)} x_i \\big)^2 - \\sum_{i=1}^{n}v_f^{(i) 2} x_i^2 \\Big) = \n",
    "\\frac{1}{2} \\sum_{f=1}^{} \\Big( S_{1,f}^2 - S_{2,f} \\Big) =\n",
    "\\frac{1}{2} \\Big( S_{1}^2 - S_{2} \\Big),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_features = X_sparse.columns[(X_sparse.nunique() == 2)]\n",
    "X_sparse = X_sparse[bin_features]\n",
    "X_sparse_tensor = torch.from_numpy(X_sparse.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8434,), (18198, 8434))"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_features.shape, X_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2weeks-granulation\t\t nohup.out\n",
      "2weeks-granulation-chernobrovov  nuxt-pwp\n",
      "adhoc\t\t\t\t polina\n",
      "base-multilabel-clf\t\t recmetrics\n",
      "blog\t\t\t\t rec-similar-dev\n",
      "course-v3\t\t\t recsys-rubbles-2019\n",
      "crm-sku\t\t\t\t test-brands-as-input\n",
      "current-baseline-model\t\t tmp\n",
      "dask-worker-space\t\t topic-modelling\n",
      "deep-fm\t\t\t\t two-step-recsys-proxy\n",
      "docs\t\t\t\t two-step-recsys-proxy-2weeks\n",
      "get-data\t\t\t two-step-recsys-with-cats\n",
      "job-interview\t\t\t two-step-recsys-with-resampling\n",
      "jupyter_local.log\t\t two-step-xdeep-2month_cat\n",
      "modules\t\t\t\t x5\n"
     ]
    }
   ],
   "source": [
    "!ls /app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/app/tmp/dota_train_binary_heroes.csv.zip', index_col='match_id_hash').astype(np.float32)\n",
    "test_df = pd.read_csv('/app/tmp/dota_train_binary_heroes.csv.zip', index_col='match_id_hash').astype(np.float32)\n",
    "target = pd.read_csv('/app/tmp/train_targets.csv.zip', index_col='match_id_hash')\n",
    "y = target['radiant_win'].values.astype(np.float32)\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39675, 230), (39675, 1))"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin_train_tensor = torch.from_numpy(y)\n",
    "X_sparse_tensor = torch.from_numpy(train_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_model(X, V, w, b):\n",
    "    s1 = ((X @ V) ** 2).sum(dim=1, keepdim=True)\n",
    "    s2 = ((X ** 2) @ (V ** 2)).sum(dim=1, keepdim=True)\n",
    "    interaction_term = (s1 - s2) / 2\n",
    "    \n",
    "    linear_term = lin_reg_model(X, w, b)\n",
    "    return sigmoid(linear_term + interaction_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# по мотивами Kaiming initialization\n",
    "w = torch.randn(y_bin_train_tensor.shape[1], X_sparse_tensor.shape[1]) / math.sqrt(2 / y_bin_train_tensor.shape[1])\n",
    "b = torch.zeros(y_bin_train_tensor.shape[1])\n",
    "\n",
    "k = 5\n",
    "V = torch.randn(X_sparse_tensor.shape[1], k)\n",
    "\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "V.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fm_model(torch.from_numpy(train_df.values), V, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff8036957f0>"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(predictions.squeeze().detach().numpy()).plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7896, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cross_entropy(predictions, sigmoid(y_bin_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.7896, grad_fn=<MulBackward0>)\n",
      "20 tensor(0.7895, grad_fn=<MulBackward0>)\n",
      "40 tensor(0.7893, grad_fn=<MulBackward0>)\n",
      "60 tensor(0.7892, grad_fn=<MulBackward0>)\n",
      "80 tensor(0.7891, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    predictions = fm_model(X_sparse_tensor, V, w, b)\n",
    "    loss = binary_cross_entropy(predictions, sigmoid(y_bin_train_tensor))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        V -= V.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        V.grad.zero_()\n",
    "    if i % 20 == 0:\n",
    "        print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fm_model(X_sparse_tensor, V, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035172323360353"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_bin_train_tensor.squeeze().detach().numpy(), predictions.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, n=None, k=None):\n",
    "        super().__init__()\n",
    "        # Initially we fill V with random values sampled from Gaussian distribution\n",
    "        # NB: use nn.Parameter to compute gradients\n",
    "        self.V = nn.Parameter(torch.randn(n, k),requires_grad=True)\n",
    "        self.lin = nn.Linear(n, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True) #S_1^2\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True) # S_2\n",
    "        \n",
    "        out_inter = 0.5*(out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 5.944705963134766\n",
      "epoch 100, loss 5.717885494232178\n",
      "epoch 200, loss 5.506598472595215\n",
      "epoch 300, loss 5.308901309967041\n",
      "epoch 400, loss 5.123404026031494\n",
      "epoch 500, loss 4.949056625366211\n",
      "epoch 600, loss 4.785022735595703\n",
      "epoch 700, loss 4.63062047958374\n",
      "epoch 800, loss 4.48528528213501\n",
      "epoch 900, loss 4.348516941070557\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "model = TorchFM(X_sparse_tensor.shape[1], 5)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Clear gradient buffers because we don't want any gradient\n",
    "    # from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    predictions = model(X_sparse_tensor)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(predictions, y_bin_train_tensor)\n",
    "    \n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5009520269804396"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(X_sparse_tensor)\n",
    "roc_auc_score(y_bin_train_tensor.squeeze().detach().numpy(), predictions.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543760900586651"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_sklearn = LogisticRegression(solver=\"liblinear\")\n",
    "log_reg_sklearn.fit(X, y);\n",
    "predicted = log_reg_sklearn.predict(X)\n",
    "roc_auc_score(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor = torch.from_numpy(y.reshape(-1, 1)).float()\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "\n",
    "# по мотивами Kaiming initialization\n",
    "w = torch.randn(y_tensor.shape[1], X_tensor.shape[1]) / math.sqrt(2 / y_tensor.shape[1])\n",
    "b = torch.zeros(y_tensor.shape[1])\n",
    "\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True);\n",
    "\n",
    "for i in range(10000):\n",
    "    predictions = log_reg_model(X_tensor, w, b)\n",
    "    loss = binary_cross_entropy(predictions, sigmoid(y_tensor))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "predictions = log_reg_model(X_tensor, w, b)\n",
    "roc_auc_score(y, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = torch.from_numpy(y.reshape(-1, 1)).float()\n",
    "X_tensor = torch.from_numpy(X).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([569, 30]), (569,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionTorch, self).__init__()\n",
    "        self.linear = torch.nn.Linear(30, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "model = LogisticRegressionTorch()\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    predictions = model(X_tensor)\n",
    "    # Compute Loss\n",
    "    loss = criterion(predictions, y_tensor)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "predictions = model(X_tensor)\n",
    "roc_auc_score(y, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b066b9a98b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    353\u001b[0m     return _average_binary_score(\n\u001b[1;32m    354\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 327\u001b[0;31m                                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \"\"\"\n\u001b[1;32m    621\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 622\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mallow_nan\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# по мотивами Kaiming initialization\n",
    "w = torch.randn(y_tensor.shape[1], X_tensor.shape[1])\n",
    "b = torch.zeros(y_tensor.shape[1])\n",
    "\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True);\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(100):\n",
    "    predictions = log_reg_model(X_tensor, w, b)\n",
    "    loss = binary_cross_entropy(predictions, sigmoid(y_tensor))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * learning_rate\n",
    "        b -= b.grad * learning_rate\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "predictions = log_reg_model(X_tensor, w, b)\n",
    "roc_auc_score(y, predictions.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

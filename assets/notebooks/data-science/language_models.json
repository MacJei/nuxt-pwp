{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Автор [Нагорный Олег](https://nagornyy.me/#/courses/DA-2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* [Language_model. Wiki](https://en.wikipedia.org/wiki/Language_model)\n",
    "* [Антон Алексеев. Введение в обработку естественного языка](https://compscicenter.ru/courses/introduction-nlp/2017-autumn/classes/)\n",
    "* [Stanford University School of Engineering. Lecture 8: Recurrent Neural Networks and Language Models](https://www.youtube.com/watch?v=Keqep_PKrY8)\n",
    "* [Сергей Николенко. Языковые модели. Конспект лекции](https://logic.pdmi.ras.ru/~sergey/teaching/asr/notes-13-langmod.pdf)\n",
    "* [Мурат Апишев. Языковые модели. Статистический машинный перевод, задача выравнивания](http://www.machinelearning.ru/wiki/images/5/5d/Mel_lain_msu_nlp_sem_2.pdf)\n",
    "* [ИСП РАН. Лекция «Языковые модели и задача определения частей речи»](http://tpc.at.ispras.ru/wp-content/uploads/2011/10/lecture3-2013.pdf)\n",
    "* [Statistical Machine Translation. Chapter 7. Language models](http://www.statmt.org/book/slides/07-language-models.pdf)\n",
    "* [Jason Brownlee. Gentle Introduction to Statistical Language Modeling and Neural Language Models](https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/)\n",
    "* Jurafsky, D. (2013). Speech and Language Processing. Pearson Education UK.\n",
    "* [Разбор статистической языковой модели от Google ](https://habr.com/company/wunderfund/blog/318454/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Языковые модели\n",
    "<div style=\"text-align: right;\">\n",
    "<div style=\"width: 50%; margin-left:auto; margin-right:0;\">\n",
    "    Вещь в себе индуктивно рассматривается конфликт, ломая рамки привычных представлений. Реальность категорически порождает и обеспечивает онтологический гравитационный парадокс. Наряду с этим мир методологически дискредитирует дуализм.\n",
    "    </div>\n",
    "<br>\n",
    "<div>\n",
    "    <i>Цитаты великих</i>\n",
    "</div>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Общая постановка задачи\n",
    "Языковые модели вычисляют вероятность заданной последовательности слов $P(w_1, w_2, \\ldots, w_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Применение\n",
    "* Оценка вероятности появления текста (можно использовать в задаче классиффикации)\n",
    "* Генерация текстов ([Яндекс.Рефераты](https://yandex.ru/referats/))\n",
    "* Машинный перевод\n",
    "    * порядок слов $p(the \\ cat \\ is \\ small) > p(small \\ the \\ is \\ cat)$\n",
    "    * выбор слов $p(walking \\ home \\ after \\ school) > p(walking \\ house \\ after \\ school)$\n",
    "* Информационный поиск. Документ соответствует поисковому запросу, если модель документа с большой вероятностью порождает этот запрос.\n",
    "* Исправление ошибок.\n",
    "* [Распознавание рукописного почерка](https://youtu.be/I674fBVanAA).\n",
    "* Чат-боты.\n",
    "* Augmentative communication (Newell, A., Langer, S., and Hickey, M. (1998). The role of natural language processing in alternative and augmentative communication. Natural Language Engineering, 4(1), 1–16.)\n",
    "* Определение частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Виды\n",
    "* классические частотные n-грамные модели\n",
    "* основанные на глубоком обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# n-грамные модели\n",
    "Мы можем оценивать вероятность слова при условии предыдущих слов. Насколько ожидаемо слово после определённой последовательности. Для понимания того, как это работает, нужно вспомнить понятие и формулу условной вероятности.\n",
    "\n",
    "$$p(y|x) = \\frac{p(x, y)}{p(y)} \\Rightarrow p(x, y) = p(y)×p(y|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Если переменных много, нужно воспользоваться правилом вычисления по цепочке:\n",
    "$$p(x_1,x_2\\ldots,x_n) = P(x_n|x_1,\\ldots,x_{n-1})\\ldots p(x_2|x_1)p(x_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "В n-грамных языковых моделях вероятность слова рассматривается как произведение условных вероятностей $P(w_1, w_2, \\ldots, w_n) = \\prod^m_{i=1}P(w_i|w_1, \\ldots, w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "На самом деле важно только некоторое количество $m$ предыдущих слов. Тогда искомую вероятность можно приблизить произведением условных вероятностьей $m$ предыдущих слов (Марковское допущение):\n",
    "\n",
    "$$P(w_1, w_2, \\ldots, w_n) = \\prod^n_{i=1}P(w_i|w_1, \\ldots, w_{i-1}) ≈ \\prod^n_{i=1}P(w_i|w_{i-(m-1)}, \\ldots, w_{i-1})$$\n",
    "\n",
    "w_{i-(m-1) называется **историей**, а сама модель — Марковской моделью m-ого порядка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "А эту вероятность можно оценить при помощи обычных частот:\n",
    "\n",
    "$$\\frac{count(w_{i-(m-1), \\ldots, w_{i-1}, w_i})}{count(w_{i-(m-1), \\ldots, w_{i-1}})}$$\n",
    "\n",
    "Оценка вероятности слова равна отношению количество случаев, когда слово встречалось после набора предыдущих из m слов, к количеству случаев появления этого самого набора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Когда $m = 2$, говорят о биграмных языковых моделях, а когда 3 — о триграмных.\n",
    "\n",
    "$$p(w_2|w_1) = \\frac{count(w_1, w_2)}{count(w_{1})} , \\\\ p(w_3|w_1, w_2)\\frac{count(w_1, w_2, w_3)}{count(w_{1}, w_{2})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Но есть нюанс...\n",
    "\n",
    "Для того, чтобы реализовать этот алгоритм, нужно добавить плейсхолдеры для начала и конца строки, количество которых будет равняться n-1, где n — количество «грамм» в модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Определите вероятность слов в биграмной модели.\n",
    "\n",
    "* ^ Я есть Грут $\n",
    "\n",
    "* ^ Есть овощи полезно $\n",
    "\n",
    "* ^ Я не люблю людей $\n",
    "\n",
    "$p(я|\\hat{}) = ?$\n",
    "\n",
    "$p(грут|есть) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Какова была бы верооятность слов каждой из фраз в униграмной языковой модели?\n",
    "\n",
    "* ^ Я есть Грут $\n",
    "\n",
    "* ^ Есть овощи полезно $\n",
    "\n",
    "* ^ Я не люблю людей $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Проблемы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* для больших m мало статистики.\n",
    "* малые m недостаточно хорошо моделируют осмысленный текст.\n",
    "* n-грамм очень-очень много, поэтому необходимы больших объёмы оперативной памяти. \n",
    "\n",
    "> Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens.\n",
    "\n",
    "*—Kenneth Heafield el al. Scalable Modified Kneser-Ney Language Model Estimation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Сглаживание\n",
    "\n",
    "Для решения проблемы ограниченности корпуса и разреженности языка применяется сглаживание, при помощи которого повышается вероятность\n",
    "некоторых n-грам, за счет понижения вероятности других."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Виды:\n",
    "* Сглаживание Лапласа (add-one)\n",
    "* Откат (backoff)\n",
    "* Интерполяция\n",
    "* Сглаживание Кнесера-Нея (Kneser-Ney)\n",
    "* Сглаживание Виттена-Белла (Witten-Bell)\n",
    "* Сглаживание Гуда-Тьюринга (Good-Turing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Сглаживание Лапласа (add-one)\n",
    "\n",
    "Добавим 1 к встречаемости каждой n-граммы.\n",
    "\n",
    "Пусть в словаре V слов, тогда\n",
    "$$p_{Laplace}(w_2|w_1) = \\frac{count(w_1, w_2) + 1}{count(w_{1}) + V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Метод интересен только с теоретической точки зрения поскольку результаты с ним часто хуже. \n",
    "\n",
    "Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Можно делать так, но тоже не сильно спасает.\n",
    "\n",
    "$$p_{Laplace}(w_2|w_1) = \\frac{count(w_1, w_2) + \\alpha}{count(w_{1}) + V\\alpha}, \\alpha\\in [0, 1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Откат (backoff)\n",
    "\n",
    "Можно использовать вероятность меньших n-грамм для вычисления больших с 0 частотой.\n",
    "\n",
    "Нет «мама мыла раму», зато есть «мама мыла»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Jelinek-Mercer smoothing (интерполяция)\n",
    "Каждую n-грамму можно рассматривать как взвешенную сумму n-1, n-2 и и т.д. грамм.\n",
    "\n",
    "$$P(w_n|w_{n−2}w_{n−1}) = λ_1P(w_n|w_{n−2}w_{n−1})\\\\+λ_2P(w_n|w_{n−1})\\\\+λ_3P(w_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Сглаживание Кнезер-Нея\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Оценка качества моделей\n",
    "\n",
    "* Внешняя. Проверка путём встраивания модели в способ решения некой полезной задачи. Если есть оптимизация целевой метрики (время переводчика, потраченное на корректировку, количество кликов на предложенный вариант исправления), то модель стала лучше\n",
    "* Внутренняя (перплексия, PMI). Не привязана к конкретной задаче.\n",
    "\n",
    "Ссылки:\n",
    "* [Внутренние критерии качества тематических моделей](https://www.coursera.org/lecture/unsupervised-learning/vnutrienniie-kritierii-kachiestva-tiematichieskikh-modieliei-OGWkI)\n",
    "* [Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Внутренние метрики\n",
    "<img src=\"img/shannon.jpg\" align=\"right\">\n",
    "Основы теории информации заложила статья 1948 г. A Mathematical Theory of Communication Клода Шеннона (1949 с комментариями ). Именно там были введены ключевые для всей информатики понятия и определены важнейшие метрики внутренней оценки качества.\n",
    "Вводятся понятия информационной энтропии, информационной избыточности, бита.\n",
    "\n",
    "<img src=\"img/information_transfer.png\" width=\"500\" style=\"float:left; padding-top:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Собственная информация\n",
    "\n",
    "Собственная информация величины X определяется как\n",
    "\n",
    "$$I(X)=-\\log P_{X}(X)$$\n",
    " \n",
    "\n",
    "Собственную информацию можно понимать как «меру неожиданности» события — чем меньше вероятность события, тем больше информации оно содержит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Единицы измерения информации зависят от основания логарифма. В случае логарифма с основанием 2 единицей измерения является бит, если используется натуральный логарифм — то нат, если десятичный — то хартли.\n",
    "\n",
    "> Логарифмическая мера более удобна по различным причинам. Она практически более пригодна. Параметры, имеющие техническое значение, такие как время, ширина полосы частот, количество реле и т.д., зависят линейно от логарифма числа возможностей. Например, добавление одного реле к некоторой схеме удваивает число возможных состояний реле. Тем самым прибавляется единица к логарифму этого числа при основании 2. Удвоение времени, грубо говоря, возводит в квадрат число возможных сообщений, т.е. удваивает логарифм и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Например, случайное событие — бросок монеты, в процессе которого выпала решка.  Сколько бит информации несёт в себе это событие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "А если кубик выпал на тройку?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.log2(6) # может ли бит быть дробным? По Шеннону да, но вообще нет\n",
    "# при равномерном распределении собственная информация — это длинна двоичного числа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Сколько информации несёт в себе предопределённый факт?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Взаимная поточечная информация (PMI)\n",
    "\n",
    "Взаимная информация — статистическая функция двух случайных величин, описывающая количество информации, содержащееся в одной случайной величине относительно другой. Взаимная поточечная информация отличается от MI тем, что она рассчитывается для пары конкретных наблюдений, а для всех возможных пар:\n",
    "\n",
    "$${pmi} (x;y)\\equiv \\log {\\frac {p(x,y)}{p(x)p(y)}}=\\log {\\frac {p(x|y)}{p(x)}}=\\log {\\frac {p(y|x)}{p(y)}}.$$\n",
    "\n",
    "* PMI говорит о том, сколько добавляется информации о слове word2, если видишь слово word1\n",
    "* Может применяться не только к подряд идущим словам\n",
    "* Дает большой вес редким словосочетаниям\n",
    "* Разумно использовать как меру независимости или, наоборот, неслучайности совместного упоминания слов/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PMI, рассчитанный на словаре Википедии.\n",
    "\n",
    "| word 1 | word 2    | count word 1 | count word 2 | count of co-occurrences | PMI           |\n",
    "|--------|-----------|--------------|--------------|-------------------------|---------------|\n",
    "| puerto | rico      | 1938         | 1311         | 1159                    | 10.0349081703 |\n",
    "| hong   | kong      | 2438         | 2694         | 2205                    | 9.72831972408 |\n",
    "| los    | angeles   | 3501         | 2808         | 2791                    | 9.56067615065 |\n",
    "| carbon | dioxide   | 4265         | 1353         | 1032                    | 9.09852946116 |\n",
    "| prize  | laureate  | 5131         | 1676         | 1210                    | 8.85870710982 |\n",
    "| san    | francisco | 5237         | 2477         | 1779                    | 8.83305176711 |\n",
    "| nobel  | prize     | 4098         | 5131         | 2498                    | 8.68948811416 |\n",
    "| ice    | hockey    | 5607         | 3002         | 1933                    | 8.6555759741  |\n",
    "| star   | trek      | 8264         | 1594         | 1489                    | 8.63974676575 |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "| to | and | 1025659 | 1375396 | 1286 | -3.08825363041 |\n",
    "| to | in  | 1025659 | 1187652 | 1066 | -3.12911348956 |\n",
    "| of | and | 1761436 | 1375396 | 1190 | -3.70663100173 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Как интерпретировать отрицательный PMI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Энтропия\n",
    "\n",
    "Энтропия — это мера неопределённости.\n",
    "\n",
    "$$H(x)=-\\sum _{i=1}^{n}p_{i}\\log _{2}p_{i}.$$\n",
    "\n",
    "Можно рассматривать как матожидание собственной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "В каком диапазоне лежит энтропия?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.113283334294875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = n\n",
    "-np.sum(1/n * np.log2([1/n, 1/n, 1/n, 1/n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Расхождение Кульбака-Лейбнера или относительная энтропия\n",
    "\n",
    "$$D_{\\text{KL}}(p\\parallel q)=-\\sum _{i}p_i\\log \\left({\\frac {q_i}{p_i}}\\right),$$\n",
    "\n",
    "В машинном обучении под $D_{\\text{KL}}(P\\parallel Q)$ часто понимают прирост информации, который был получен, когда вместо P использовали Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Кросс-энтропия\n",
    "\n",
    "Перекрёстная энтропия между двумя распределениями вероятностей измеряет среднее число бит, необходимых для опознания события из набора возможностей, если используемая схема кодирования базируется на заданном распределении вероятностей q, вместо «истинного» распределения p.\n",
    "\n",
    "$$\\mathrm {H} (p,q){\\stackrel {\\mathrm {df} }{\\;=\\;}}\\mathrm {E} _{p}[-\\log q]=\\mathrm {H} (p)+D_{\\mathrm {KL} }(p\\|q) =-\\sum _{x}p\\,\\log q.\\!$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Кстати, помните [logloss для бинарной классификации из предыдущего занятия](https://i-hun.github.io/da_in_special_enviroments_2018/4_metrics/index.html#/61)?\n",
    "\n",
    "Давайте рассмотрим конкретный объект x с меткой y. Если алгоритм выдаёт вероятность принадлежности первому классу – $\\hat{y_i}$, то предполагаемое распределение на событиях «класс 0», «класс 1» – (1–$\\hat{y_i}$, $\\hat{y_i}$), а истинное – (1–y, y), поэтому кросс-энтропия между ними\n",
    "\n",
    "$$-(y_i\\log(\\hat{y_i}) + (1 - y_i)\\log(1 - \\hat{y_i})),$$\n",
    "\n",
    "что в точности logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Энтропия последовательности](https://en.wikipedia.org/wiki/Joint_entropy)\n",
    "\n",
    "Считаем все возможные энтропии и суммируем.\n",
    "\n",
    "$$\\mathrm {H} (X_{1},...,X_{n})=-\\sum _{x_{1}}...\\sum _{x_{n}}P(x_{1},...,x_{n})\\log _{2}[P(x_{1},...,x_{n})]$$\n",
    "\n",
    "Если n очень большое, то посчитать это сложно. К счастью есть [Теорема Шеннона–Макмиллана–Бреймана](https://en.wikipedia.org/wiki/Asymptotic_equipartition_property), которая говорит, что для стационарной и эрдогической последовательности можно взять не все элементы последовательности, а достаточно большое их число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "http://www.wikiznanie.ru/wikipedia/index.php/%D0%AD%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F_(%D0%B2_%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D0%B8_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8)#.D0.92.D0.B7.D0.B0.D0.B8.D0.BC.D0.BD.D0.B0.D1.8F_.D1.8D.D0.BD.D1.82.D1.80.D0.BE.D0.BF.D0.B8.D1.8F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Перплексия\n",
    "\n",
    "$$2^{{H(p)}}=2^{{-\\sum _{x}p(x)\\log _{2}p(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "https://dyakonov.org/2018/03/12/%D0%BB%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F-%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Языковые модели и байесовский статистический вывод\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Языковые модели, основанные на моделях глубокого обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) ([по-русски](https://habr.com/company/wunderfund/blog/331310/))\n",
    "* []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Практика\n",
    "\n",
    "* [markovify](https://github.com/jsvine/markovify)\n",
    "* [Генерируем заголовки фейковых новостей в стиле Ленты.ру](https://habr.com/post/345190/)\n",
    "* [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n",
    "* [Neural text generation: How to generate text using conditional language models](https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b)\n",
    "* [Перенос авторского стиля через нейросеть. Как создавался «Нейромирон»](https://vas3k.ru/blog/394/)\n",
    "* [How to create a poet / writer using Deep Learning (Text Generation using Python)?](https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/)\n",
    "* [A Hands-on Tutorial on Neural Language Models](https://dashayushman.github.io/tutorials/2017/08/19/neural-language-model.html)\n",
    "* [nltk.model package](http://www.nltk.org/api/nltk.model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Практическая часть](https://github.com/i-Hun/da_in_special_enviroments_2018/blob/master/6_text_models/6_text_models_practice.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
